import { useCallback, useEffect, useRef, useState } from "react";

import type {
  AudioProcessingResult,
  AudioError,
  ChunkData,
  WorkletMessage,
  WorkletMessagePayload,
} from "../utils/types";

interface AudioWorkletState {
  isRecording: boolean;
  chunks: ChunkData[];
  totalChunks: number;
  recordedFrames: number;
}

export function useAudioWorklet(audioContext: AudioContext | null) {
  const [workletState, setWorkletState] = useState<AudioWorkletState>({
    isRecording: false,
    chunks: [],
    totalChunks: 0,
    recordedFrames: 0,
  });

  const workletNodeRef = useRef<AudioWorkletNode | null>(null);
  const sourceNodeRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const workletStateRef = useRef(workletState);
  const handleWorkletMessageRef = useRef<(message: WorkletMessage) => void>(
    () => {},
  );

  // workletStateRef„ÇíÊúÄÊñ∞„ÅÆÁä∂ÊÖã„Å´Êõ¥Êñ∞
  useEffect(() => {
    workletStateRef.current = workletState;
  }, [workletState]);

  // AudioWorklet„Åã„Çâ„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏Âá¶ÁêÜ
  const handleWorkletMessage = useCallback(
    (message: WorkletMessage) => {
      switch (message.type) {
        case "recording-started":
          console.log("üì° Recording started:", message.payload);
          break;

        case "chunk": {
          const payload = message.payload as WorkletMessagePayload<"chunk">;
          if (!audioContext) return;

          const chunkData: ChunkData = {
            id: payload.chunkIndex,
            startTime: payload.timestamp,
            endTime:
              payload.timestamp +
              payload.audioData.length / audioContext.sampleRate,
            data: payload.audioData,
            isSelected: false,
          };

          setWorkletState((prev) => {
            const newState = {
              ...prev,
              chunks: [...prev.chunks, chunkData],
            };
            console.log(`üîÑ State updated: chunks=${newState.chunks.length}`);
            return newState;
          });

          console.log(`üì¶ Chunk ${payload.chunkIndex} received:`, {
            size: payload.audioData.length,
            min: payload.minValue,
            max: payload.maxValue,
          });
          break;
        }

        case "recording-stopped": {
          const payload =
            message.payload as WorkletMessagePayload<"recording-stopped">;
          console.log("üì° Recording stopped:", payload);
          setWorkletState((prev) => ({
            ...prev,
            recordedFrames: payload.totalFrames,
          }));
          break;
        }
        default:
          console.warn("Unknown worklet message:", message.type);
      }
    },
    [audioContext],
  );

  // handleWorkletMessage„ÇíuseRef„Å´‰øùÂ≠ò
  handleWorkletMessageRef.current = handleWorkletMessage;

  // AudioWorkletProcessor„ÇíÂàùÊúüÂåñ
  const initializeWorklet = useCallback(async (): Promise<
    AudioProcessingResult<void>
  > => {
    try {
      if (!audioContext) {
        throw new Error("Audio context not initialized");
      }

      // AudioWorkletProcessor„ÇíË™≠„ÅøËæº„Åø
      const workletPath = new URL(
        "../worklets/recording-processor.js",
        import.meta.url,
      );
      await audioContext.audioWorklet.addModule(workletPath);

      console.log("‚úÖ AudioWorkletProcessor loaded successfully");

      return { success: true };
    } catch (error) {
      const audioError: AudioError = {
        code: "WORKLET_INITIALIZATION_ERROR",
        message:
          error instanceof Error
            ? error.message
            : "Failed to initialize AudioWorklet",
        details: { error },
      };

      return { success: false, error: audioError };
    }
  }, [audioContext]);

  // Èå≤Èü≥ÈñãÂßãÔºàAudioWorklet„Çí‰ΩøÁî®Ôºâ
  const startWorkletRecording = useCallback(
    async (duration: number = 2.0): Promise<AudioProcessingResult<void>> => {
      try {
        if (!audioContext) {
          throw new Error("Audio context not initialized");
        }

        if (workletState.isRecording) {
          return {
            success: false,
            error: {
              code: "RECORDING_ALREADY_IN_PROGRESS",
              message: "Recording is already in progress",
            },
          };
        }

        // „Éû„Ç§„ÇØ„Ç¢„ÇØ„Çª„Çπ„ÇíÂèñÂæó
        const stream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: false,
            noiseSuppression: false,
            autoGainControl: false,
            sampleRate: audioContext.sampleRate,
          },
        });

        // AudioWorkletNode‰ΩúÊàê
        const workletNode = new AudioWorkletNode(
          audioContext,
          "recording-processor",
          {
            numberOfInputs: 1,
            numberOfOutputs: 1,
            channelCount: 1,
            channelCountMode: "explicit",
            channelInterpretation: "speakers",
          },
        );

        // MediaStreamAudioSourceNode‰ΩúÊàê
        const sourceNode = audioContext.createMediaStreamSource(stream);

        // „É°„ÉÉ„Çª„Éº„Ç∏„Éè„É≥„Éâ„É©„ÉºË®≠ÂÆö
        workletNode.port.onmessage = (event: MessageEvent<WorkletMessage>) => {
          handleWorkletMessageRef.current(event.data);
        };

        // „Éé„Éº„ÉâÊé•Á∂ö
        sourceNode.connect(workletNode);

        // ÂèÇÁÖß„Çí‰øùÂ≠ò
        workletNodeRef.current = workletNode;
        sourceNodeRef.current = sourceNode;
        mediaStreamRef.current = stream;

        // Èå≤Èü≥ÈñãÂßã
        workletNode.port.postMessage({
          type: "start",
          payload: { duration },
        });

        setWorkletState((prev) => ({
          ...prev,
          isRecording: true,
          chunks: [],
          totalChunks: 150,
          recordedFrames: 0,
        }));

        console.log("üé§ AudioWorklet recording started:", { duration });

        return { success: true };
      } catch (error) {
        const audioError: AudioError = {
          code: "WORKLET_RECORDING_START_ERROR",
          message:
            error instanceof Error
              ? error.message
              : "Failed to start worklet recording",
          details: { error },
        };

        return { success: false, error: audioError };
      }
    },
    [audioContext, workletState.isRecording],
  );

  // Èå≤Èü≥ÂÅúÊ≠¢
  const stopWorkletRecording = useCallback((): AudioProcessingResult<void> => {
    try {
      if (!workletStateRef.current.isRecording) {
        return {
          success: false,
          error: {
            code: "RECORDING_NOT_IN_PROGRESS",
            message: "Recording is not in progress",
          },
        };
      }

      // Èå≤Èü≥ÂÅúÊ≠¢
      if (workletNodeRef.current) {
        workletNodeRef.current.port.postMessage({ type: "stop" });
      }

      // „Éé„Éº„ÉâÂàáÊñ≠
      if (sourceNodeRef.current) {
        sourceNodeRef.current.disconnect();
      }

      if (workletNodeRef.current) {
        workletNodeRef.current.disconnect();
      }

      // MediaStream„ÇíÂÅúÊ≠¢
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach((track) => track.stop());
      }

      // ÂèÇÁÖß„Çí„ÇØ„É™„Ç¢
      workletNodeRef.current = null;
      sourceNodeRef.current = null;
      mediaStreamRef.current = null;

      setWorkletState((prev) => ({
        ...prev,
        isRecording: false,
      }));

      console.log("üõë AudioWorklet recording stopped");

      return { success: true };
    } catch (error) {
      const audioError: AudioError = {
        code: "WORKLET_RECORDING_STOP_ERROR",
        message:
          error instanceof Error
            ? error.message
            : "Failed to stop worklet recording",
        details: { error },
      };

      return { success: false, error: audioError };
    }
  }, []);

  // „ÉÅ„É£„É≥„ÇØ„Åã„ÇâAudioBuffer„Çí‰ΩúÊàê
  const createAudioBufferFromChunks = useCallback((): AudioBuffer | null => {
    if (!audioContext || workletStateRef.current.chunks.length === 0) {
      return null;
    }

    try {
      // ÂÖ®„ÉÅ„É£„É≥„ÇØ„ÅÆ„Çµ„É≥„Éó„É´Êï∞„ÇíË®àÁÆó
      const totalSamples = workletStateRef.current.chunks.reduce(
        (sum, chunk) => sum + chunk.data.length,
        0,
      );

      // AudioBuffer„Çí‰ΩúÊàê
      const audioBuffer = audioContext.createBuffer(
        1, // „É¢„Éé„É©„É´
        totalSamples,
        audioContext.sampleRate,
      );

      // „ÉÅ„É£„É≥„ÇØ„Éá„Éº„Çø„ÇíÁµêÂêà
      const channelData = audioBuffer.getChannelData(0);
      let offset = 0;

      for (const chunk of workletStateRef.current.chunks) {
        channelData.set(chunk.data, offset);
        offset += chunk.data.length;
      }

      console.log("üîä AudioBuffer created from chunks:", {
        totalSamples,
        duration: audioBuffer.duration,
        chunkCount: workletStateRef.current.chunks.length,
      });

      return audioBuffer;
    } catch (error) {
      console.error("‚ùå Failed to create AudioBuffer from chunks:", error);
      return null;
    }
  }, [audioContext]);

  // „ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
  const cleanup = useCallback(() => {
    if (workletStateRef.current.isRecording) {
      stopWorkletRecording();
    }
  }, [stopWorkletRecording]);

  return {
    workletState,
    initializeWorklet,
    startWorkletRecording,
    stopWorkletRecording,
    createAudioBufferFromChunks,
    cleanup,
  };
}
