# Collidoscope Web版 実装仕様

## 概要

オリジナルのCollidoscopeは、Raspberry Pi上で動作するC++製のアプリケーションと、Teensyマイクロコントローラーで制御される物理的なインターフェースで構成される楽器です。このドキュメントでは、その機能をWeb技術（React, TypeScript, Web Audio API）を用いて再現するための仕様を定義します。

**開発は段階的に行い、まずは単一のシンセエンジンを実装します。コンポーネントは再利用可能な形で設計し、最終的に2つ目のシンセエンジンを並べて配置できるように拡張可能にします。**

## 1. 音声入力

### 1.1. マイク入力

- **機能:** ユーザーのマイクから音声を取り込み、録音機能に渡します。
- **実装:**
  - `navigator.mediaDevices.getUserMedia` を使用してマイクへのアクセス許可をユーザーに求めます。
  - 許可された場合、マイクからの音声ストリームをWeb Audio APIの`MediaStreamAudioSourceNode`に接続します。
  - 録音機能が有効化された際に、このストリームを録音処理に渡します。

## 2. 音声録音と波形表示

### 2.1. 録音機能

- **機能:** マイクから入力された音声を指定された時間録音し、音声バッファに保存します。録音時間は設定可能です。
- **実装:**
  - 録音時間（デフォルト2秒）を設定ファイルまたはUIから変更可能な変数として保持します。
  - Web Audio APIの`MediaRecorder` APIまたは`AudioWorkletNode`を用いて、マイクからの音声を録音します。
  - 録音された音声データは`AudioBuffer`として保持します。
  - 録音ボタンが押されると、既存の録音データを破棄し、新しい録音を開始します。

### 2.2. 波形表示 (Wave)

- **機能:** 録音された音声データを視覚的な波形として画面に表示します。**初期実装では単一の波形を表示し、将来的にはオリジナル同様、2つの独立した波形（Wave 1: 赤、Wave 2: 黄）を表示できるように設計します。**
- **実装:**
  - 録音された`AudioBuffer`のデータを解析し、波形を描画します。
  - オリジナルの「チャンク」スタイルを再現するため、波形を一定数のブロックに分割して、各ブロックの最大振幅と最小振幅をバーとして描画します。
  - `requestAnimationFrame`を使用して、滑らかな描画を実現します。
  - （将来的な拡張として）Wave 2は上下反転して表示します。

### 2.3. オシロスコープ表示

- **機能:** 現在再生されている音声の波形をリアルタイムで表示します。
- **実装:**
  - Web Audio APIの`AnalyserNode`を使用して、再生中の音声データの波形情報を取得します。
  - `requestAnimationFrame`ループ内で`getByteTimeDomainData`を呼び出し、取得したデータを`<canvas>`要素に描画します。

## 3. グラニュラーシンセシスエンジン

Collidoscopeの心臓部であるグラニュラーシンセサイザーをWeb Audio APIで再現します。

### 3.1. 音声の選択 (Selection)

- **機能:** ユーザーが録音された波形の一部を選択できるようにします。
- **実装:**
  - **UI:** オリジナルの操作感を再現するため、波形表示エリアの下に、ノブ付きの水平スライダーUIを実装します。
    - **選択範囲の開始位置 (Selection Start):** スライダーのノブ部分を水平にドラッグすることで制御します。
    - **選択範囲のサイズ (Selection Size):** スライダーのノブにマウスカーソルを乗せた状態で、マウスホイールを上下に回転させることで制御します。
  - **マッピング:**
    - 開始位置は0-149の範囲にマッピングします。
    - サイズは1-37（チャンク数）の範囲にマッピングします。

### 3.2. グレインの生成と再生 (Grain Engine)

- **機能:** 選択された音声範囲から「グレイン（音の粒）」を生成し、再生します。
- **実装:**
  - `AudioBufferSourceNode`を使用して、選択範囲の音声を再生します。
  - 以下のパラメータをWeb UIで制御できるようにします。
    - **ピッチ (Pitch):**
      - ピアノキーボードUIまたはPCのキーボード入力にマッピングします。
      - `AudioBufferSourceNode.playbackRate`プロパティを変更することでピッチを変化させます。
    - **グレインの持続時間 (Grain Duration):**
      - スライダーUIで制御します（1〜8倍）。
      - この値が1より大きい場合、複数のグレインがオーバーラップして再生され、独特のテクスチャを生み出します。`GainNode`と`AudioBufferSourceNode`を複数生成し、タイミングをずらして再生することで実現します。
    - **ループ (Loop):**
      - ON/OFFを切り替えるトグルボタンを設置します。
      - ONの場合、選択範囲の再生をループさせます。`AudioBufferSourceNode.loop`プロパティを`true`に設定します。
    - **フィルター (Filter):**
      - ローパスフィルターのカットオフ周波数をスライダーUIで制御します（200Hz〜22050Hz）。
      - `BiquadFilterNode`を使用し、`type`を`lowpass`に設定して実装します。カットオフ周波数は`frequency`パラメータで制御します。

### 3.3. 再生カーソルとパーティクル表示

- **機能:**
  - ループ再生中に、現在の再生位置を示すカーソルを選択範囲上に表示します。
  - グレインの持続時間が長い場合に、視覚的なフィードバックとしてパーティクルを放出します。
- **実装:**
  - **カーソル:** `requestAnimationFrame`ループ内で、現在の再生時間に基づいてカーソルの位置を計算し、`<canvas>`上に描画します。
  - **パーティクル:** グレインの持続時間（`duration`）が1より大きい場合に、その値に応じてパーティクルの量や広がりを変化させて描画します。

## 4. ユーザーインターフェース (UI)

### 4.1. 全体レイアウト

- **初期実装では、単一のシンセエンジンを画面中央に配置します。**
- 各エンジンには、波形表示エリア、ピアノキーボード、各種コントロール（スライダー、ボタン）を配置します。
- 将来的には、オリジナルの物理的なレイアウトを参考に、2つのシンセエンジンを左右（または上下）に配置できるように拡張します。

### 4.2. UIコンポーネント

- **ピアノキーボード:**
  - 画面上のキーをクリック、またはPCのキーボード（例: A,S,D,F...）を押すことで、対応するMIDIノートメッセージを生成し、シンセエンジンに送信します。
- **スライダー/ノブ:**
  - `Selection Start`, `Selection Size`, `Duration`, `Filter`の各パラメータを制御するために、React製のスライダーコンポーネントを使用します。
  - マウスドラッグやホイール操作で値を変更できるようにします。
- **ボタン:**
  - `Record`, `Loop On/Off`を制御するボタンを設置します。

## 5. 技術スタック

- **フロントエンド:** React, TypeScript
- **音声処理:** Web Audio API
- **UIコンポーネント:** MUI v7
- **状態管理:** 必要に応じて Zustand
